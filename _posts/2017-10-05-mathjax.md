Skip to content
This repository
Search
Pull requests
Issues
Marketplace
Explore
 @pltrdy
 Sign out
 Unwatch 1
  Star 0
 Fork 2,155 pltrdy/beautiful-jekyll
forked from daattali/beautiful-jekyll
 Code  Pull requests 0  Projects 0  Wiki  Settings Insights 
Branch: master Find file Copy pathbeautiful-jekyll/_posts/2017-10-05-mathjax
2dde772  2 minutes ago
@pltrdy pltrdy Create 2017-10-05-mathjax
1 contributor
RawBlameHistory    
26 lines (18 sloc)  862 Bytes
---
layout: post
title: Testing MatJax
subtitle: Adding LaTeX equations in Markdown
---

## Using double dollar notation:   

    $$  e_i^t = v^T tanh(W_ h_i + W_s s_t + b_{attn}) $$
    $$  a^t = softmax(e^t) $$ 
    $$ h_t^* = \sum_i{a_i^th_i} $
    $$ P_{vocab}= softmax(V'(V[s_t, h_t^*]+b)+b')$$
    $$ loss_t = -log P(w_t^*)$$
   
## Using single dollar notation (inline):
Attention score is then $  e_i^t = v^T tanh(W_ h_i + W_s s_t + b_{attn}) $, we note $a^t$ the corresponding probabilities i.e. $  a^t = softmax(e^t) $

## Using double backslashes+bracket notation:
\\[ e_i^t = v^T tanh(W_ h_i + W_s s_t + b_{attn})  \\]
\\[  a^t = softmax(e^t) \\]

## Using double backslashes+parentheses:
Attention score is then \\(  e_i^t = v^T tanh(W_ h_i + W_s s_t + b_{attn})\\), we note \\(a^t\\) the corresponding probabilities i.e. \\(a^t = softmax(e^t)\\)
